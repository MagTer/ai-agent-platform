============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.0, pluggy-1.6.0 -- /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/bin/python
cachedir: .pytest_cache
rootdir: /home/magnus/dev/ai-agent-platform/services/agent
configfile: pyproject.toml
plugins: respx-0.22.0, anyio-4.12.0, langsmith-0.5.1, asyncio-1.3.0
asyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 26 items

src/core/tests/test_agent_scenarios.py::test_run_tool_flow FAILED        [  3%]
src/core/tests/test_app.py::test_chat_completions_roundtrip PASSED       [  7%]
src/core/tests/test_config.py::test_settings_env_override PASSED         [ 11%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_validate_path_valid PASSED [ 15%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_validate_path_traversal PASSED [ 19%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_ls_happy_path PASSED [ 23%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_ls_security_block PASSED [ 26%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_read_happy_path PASSED [ 30%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_read_truncation PASSED [ 34%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_read_security_block PASSED [ 38%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_edit_file_success PASSED [ 42%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_edit_file_not_found PASSED [ 46%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_edit_file_ambiguous PASSED [ 50%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_edit_file_fuzzy_success PASSED [ 53%]
src/core/tests/test_filesystem.py::TestFilesystemTools::test_edit_file_fuzzy_fail_structure PASSED [ 57%]
src/core/tests/test_memory.py::test_add_records_generates_unique_point_ids PASSED [ 61%]
src/core/tests/test_memory.py::test_search_returns_all_payload_matches PASSED [ 65%]
src/core/tests/test_memory.py::test_search_supports_conversation_filter PASSED [ 69%]
src/core/tests/test_qa.py::test_qa_tools PASSED                          [ 73%]
src/core/tests/test_service.py::test_agent_service_roundtrip PASSED      [ 76%]
src/core/tests/test_service.py::test_plan_driven_flow PASSED             [ 80%]
src/core/tests/test_tools.py::test_load_tool_registry_registers_tools PASSED [ 84%]
src/core/tests/test_tools.py::test_load_tool_registry_handles_missing_file PASSED [ 88%]
src/core/tests/test_tools.py::test_agent_service_executes_tool PASSED    [ 92%]
src/core/tests/test_tools.py::test_web_fetch_tool_parses_fetcher_response PASSED [ 96%]
src/core/tests/test_tools.py::test_web_fetch_tool_raises_on_error_response PASSED [100%]

=================================== FAILURES ===================================
______________________________ test_run_tool_flow ______________________________

mock_agent_service = <core.core.service.AgentService object at 0x7fc31f4cf6e0>
mock_litellm = <core.tests.mocks.MockLLMClient object at 0x7fc31f4cf740>
tmp_path = PosixPath('/tmp/pytest-of-magnus/pytest-109/test_run_tool_flow0')

    @pytest.mark.asyncio
    async def test_run_tool_flow(
        mock_agent_service: AgentService, mock_litellm: MockLLMClient, tmp_path: Path
    ) -> None:
        """
        Scenario: User asks to read a file.
        1. Mock Planner returns a plan to use 'read_file'.
        2. Agent executes 'read_file'.
        3. Mock Responder returns the answer.
        """
        # Setup: Create a real file to read
        test_file = tmp_path / "hello.txt"
        test_file.write_text("Hello World!")
    
        # Needs absolute path for tool
        abs_path = str(test_file.resolve())
    
        # Mock Planner Response (JSON Plan)
        plan_json = {
            "description": "Read the file and answer.",
            "steps": [
                {
                    "id": "step-1",
                    "label": "Read File",
                    "executor": "agent",
                    "action": "tool",
                    "tool": "read_file",
                    "args": {"path": abs_path},
                },
                {
                    "id": "step-2",
                    "label": "Answer",
                    "executor": "litellm",
                    "action": "completion",
                    "args": {"model": "mock-model"},
                },
            ],
        }
    
        # Mock Responder Response (Final Answer)
        final_answer = "The file contains: Hello World!"
    
        # Queue responses:
        # 1. Planner Agent call -> returns plan_json
        # 2. Plan Supervisor review -> returns approval (implicitly handled if not mocked,
        #    but let's assume supervisor uses same LLM?)
        #    Actually supervisor prompts LLM "Review this plan...".
        #    Since we share one MockLLM, we need to queue logic carefully or make MockLLM smarter.
        #    For simplicity, let's just queue responses in order.
        #    Sequence:
        #    1. Planner -> Plan
        #    2. Supervisor (Review Plan) -> "looks good" (text)
        #    3. Step Supervisor (Review Step 1 result) -> "continue" (text) - wait, this might be loop
        #    4. Responder -> Final Answer
    
        # To make this robust, MockLLM in real world usually checks prompt content.
        # For now, let's queue enough "clean" responses.
    
        responses: list[str | dict[str, Any]] = [
            json.dumps(plan_json),  # 1. Planner
            final_answer,  # 2. Step 2 Execution (Completion)
        ]
    
        # Override the mock_litellm responses
        mock_litellm.responses = responses
        # Reset index
        mock_litellm._response_index = 0
    
        # Execute
        request = AgentRequest(prompt=f"Read {abs_path}", conversation_id=str(uuid.uuid4()))
    
        # We need a db session. Conftest usually provides one, but we didn't add it.
        # Let's mock the session or use an in-memory sqlite if possible?
        # Our `AgentService.handle_request` requires `session: AsyncSession`.
        # Let's create a dummy AsyncMock for the session since we don't test DB persistence here
        # strictly, OR we use a real in-memory SQLite if `core.db.engine` allows.
        # Given the constraints, let's use `unittest.mock.AsyncMock` for session.
    
        from unittest.mock import AsyncMock, MagicMock
    
        mock_session = AsyncMock()
        # We need to handle `await session.get(Conversation, ...)` -> return None (trigger creation)
        # `session.execute(...)` -> result.scalar_one_or_none()
    
        # This is getting complex to mock fully without a real DB fixture.
        # Strategy: Mock `session.get` to return a dummy Conversation,
        # `session.execute` to return dummy Context.
    
        # Mock Context retrieval
        mock_context = MagicMock()
        mock_context.id = uuid.uuid4()
        mock_context.default_cwd = str(tmp_path)
        mock_context.pinned_files = []
    
        mock_conversation = MagicMock()
        mock_conversation.context_id = mock_context.id
        mock_conversation.current_cwd = str(tmp_path)
    
        # session.get(Conversation, ...) -> None first time? Or let's imply it exists.
        # session.get(Context, ...) -> mock_context
    
        async def side_effect_get(model: Any, id: Any) -> Any:
            if str(model.__name__) == "Conversation":
                return mock_conversation
            if str(model.__name__) == "Context":
                return mock_context
            return None
    
        mock_session.get.side_effect = side_effect_get
    
        # session.execute() for Session/History lookup
        # This is tricky.
        # Let's simplify: Mock the `AgentService._memory` and bypass DB logic if possible?
        # No, `handle_request` is monolithic and hits DB.
        # We really should have an in-memory SQLITE fixture.
        # But for now, let's try to mock the specific calls `handle_request` makes.
        # Or better: Add a `db_session` fixture that uses `sqlite+aiosqlite:///:memory:`
        # (requires `aiosqlite` dep).
        # Checking pyproject.toml -> `asyncpg` is used. `aiosqlite` not listed.
        # We can't use sqlite easily.
    
        # Fallback: Mock `session.execute` to return object with `.scalars().all()` returning
        # empty list (no history).
        mock_result = MagicMock()
        mock_result.scalar_one_or_none.return_value = None  # For session lookup -> triggers new session
        mock_result.scalars.return_value.all.return_value = []  # No history
    
        mock_session.execute.return_value = mock_result
    
        # Running the service
>       response = await mock_agent_service.handle_request(request, session=mock_session)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

src/core/tests/test_agent_scenarios.py:143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/core/service.py:509: in handle_request
    async for chunk in self.execute_stream(request, session):
src/core/core/service.py:274: in execute_stream
    async for event in planner.generate_stream(
src/core/agents/planner.py:212: in generate_stream
    async for chunk in self._litellm.stream_chat(msgs, model=model_name):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <core.tests.mocks.MockLLMClient object at 0x7fc31f4cf740>
messages = [AgentMessage(role='system', content='SYSTEM CONTEXT:\n- Current Date & Time: 2025-12-29 18:23\n- Your knowledge cutof...flow0/hello.txt\n\n### METADATA\n{}\n\nCreate the execution plan now.", name=None, tool_calls=None, tool_call_id=None)]

    async def stream_chat(
        self, messages: Iterable[AgentMessage], *, model: str | None = None
    ) -> AsyncGenerator[AgentChunk, None]:
        """Stream chat completions from LiteLLM."""
        payload: dict[str, Any] = {
            "model": model or self._settings.litellm_model,
            "messages": [message.model_dump() for message in messages],
            "stream": True,
        }
        start_time = time.perf_counter()
        first_token_received = False
    
        try:
>           async with self._client.stream(
                       ^^^^^^^^^^^^
                "POST",
                "/v1/chat/completions",
                json=payload,
                headers=self._build_headers(),
            ) as response:
E           AttributeError: 'MockLLMClient' object has no attribute '_client'

src/core/core/litellm_client.py:61: AttributeError
=============================== warnings summary ===============================
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:132: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(db_session)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_plan_driven_flow
src/core/tests/test_tools.py::test_agent_service_executes_tool
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:206: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_app.py::test_chat_completions_roundtrip
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/app.py:183: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_app.py::test_chat_completions_roundtrip
  /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/fastapi/applications.py:4575: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

src/core/tests/test_app.py::test_chat_completions_roundtrip
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/app.py:203: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_plan_driven_flow
src/core/tests/test_tools.py::test_agent_service_executes_tool
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:102: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(db_conversation)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_app.py: 16 warnings
src/core/tests/test_service.py: 27 warnings
src/core/tests/test_tools.py: 9 warnings
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/observability/tracing.py:245: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    span.set_attribute("span.end_time", datetime.utcnow().isoformat())

src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_app.py::test_chat_completions_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_agent_service_roundtrip
src/core/tests/test_service.py::test_plan_driven_flow
src/core/tests/test_tools.py::test_agent_service_executes_tool
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:463: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_service.py::test_plan_driven_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:438: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED src/core/tests/test_agent_scenarios.py::test_run_tool_flow - Attribute...
================== 1 failed, 25 passed, 77 warnings in 0.29s ===================
