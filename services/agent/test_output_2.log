============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.0, pluggy-1.6.0 -- /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/bin/python
cachedir: .pytest_cache
rootdir: /home/magnus/dev/ai-agent-platform/services/agent
configfile: pyproject.toml
plugins: langsmith-0.5.0, respx-0.22.0, anyio-4.12.0, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

src/core/tests/test_agent_scenarios.py::test_run_tool_flow FAILED        [100%]

=================================== FAILURES ===================================
______________________________ test_run_tool_flow ______________________________

mock_agent_service = <core.core.service.AgentService object at 0x7fb56edb10d0>
mock_litellm = <core.tests.mocks.MockLLMClient object at 0x7fb56edb1250>
tmp_path = PosixPath('/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0')

    @pytest.mark.asyncio
    async def test_run_tool_flow(mock_agent_service, mock_litellm, tmp_path):
        """
        Scenario: User asks to read a file.
        1. Mock Planner returns a plan to use 'read_file'.
        2. Agent executes 'read_file'.
        3. Mock Responder returns the answer.
        """
        # Setup: Create a real file to read
        test_file = tmp_path / "hello.txt"
        test_file.write_text("Hello World!")
    
        # Needs absolute path for tool
        abs_path = str(test_file.resolve())
    
        # Mock Planner Response (JSON Plan)
        plan_json = {
            "description": "Read the file and answer.",
            "steps": [
                {
                    "id": "step-1",
                    "label": "Read File",
                    "executor": "agent",
                    "action": "tool",
                    "tool": "read_file",
                    "args": {"path": abs_path}
                },
                {
                    "id": "step-2",
                    "label": "Answer",
                    "executor": "litellm",
                    "action": "completion",
                    "args": {"model": "mock-model"}
                }
            ]
        }
    
        # Mock Responder Response (Final Answer)
        final_answer = f"The file contains: Hello World!"
    
        # Queue responses:
        # 1. Planner Agent call -> returns plan_json
        # 2. Plan Supervisor review -> returns approval (implicitly handled if not mocked, but let's assume supervisor uses same LLM?)
        #    Actually supervisor prompts LLM "Review this plan...". Since we share one MockLLM, we need to queue logic carefully or make MockLLM smarter.
        #    For simplicity, let's just queue responses in order.
        #    Sequence:
        #    1. Planner -> Plan
        #    2. Supervisor (Review Plan) -> "looks good" (text)
        #    3. Step Supervisor (Review Step 1 result) -> "continue" (text) - wait, this might be loop
        #    4. Responder -> Final Answer
    
        # To make this robust, MockLLM in real world usually checks prompt content.
        # For now, let's queue enough "clean" responses.
    
        responses = [
            json.dumps(plan_json),          # Planner
            "yes",                          # Plan Supervisor (simple 'yes' might fail strict parsing, let's see default)
                                            # Supervisor expects "APPROVE" or JSON? Need to check.
                                            # Let's inspect `PlanSupervisorAgent.review` logic if failing.
                                            # Assuming "looks good" passes.
            "looks good",
            "looks good",                   # Step Supervisor
            final_answer                    # Responder / Completion Step
        ]
    
        # Override the mock_litellm responses
        mock_litellm.responses = responses
        # Reset index
        mock_litellm._response_index = 0
    
        # Execute
        request = AgentRequest(
            prompt=f"Read {abs_path}",
            conversation_id=str(uuid.uuid4())
        )
    
        # We need a db session. Conftest usually provides one, but we didn't add it.
        # Let's mock the session or use an in-memory sqlite if possible?
        # Our `AgentService.handle_request` requires `session: AsyncSession`.
        # Let's create a dummy AsyncMock for the session since we don't test DB persistence here strictly,
        # OR we use a real in-memory SQLite if `core.db.engine` allows.
        # Given the constraints, let's use `unittest.mock.AsyncMock` for session.
    
        from unittest.mock import AsyncMock, MagicMock
        mock_session = AsyncMock()
        # We need to handle `await session.get(Conversation, ...)` -> return None (trigger creation)
        # `session.execute(...)` -> result.scalar_one_or_none()
    
        # This is getting complex to mock fully without a real DB fixture.
        # Strategy: Mock `session.get` to return a dummy Conversation, `session.execute` to return dummy Context.
    
        # Mock Context retrieval
        mock_context = MagicMock()
        mock_context.id = uuid.uuid4()
        mock_context.default_cwd = str(tmp_path)
        mock_context.pinned_files = []
    
        mock_conversation = MagicMock()
        mock_conversation.context_id = mock_context.id
        mock_conversation.current_cwd = str(tmp_path)
    
        # session.get(Conversation, ...) -> None first time? Or let's imply it exists.
        # session.get(Context, ...) -> mock_context
    
        async def side_effect_get(model, id):
            if str(model.__name__) == "Conversation":
                return mock_conversation
            if str(model.__name__) == "Context":
                return mock_context
            return None
    
        mock_session.get.side_effect = side_effect_get
    
        # session.execute() for Session/History lookup
        # This is tricky.
        # Let's simplify: Mock the `AgentService._memory` and bypass DB logic if possible?
        # No, `handle_request` is monolithic and hits DB.
        # We really should have an in-memory SQLITE fixture.
        # But for now, let's try to mock the specific calls `handle_request` makes.
        # Or better: Add a `db_session` fixture that uses `sqlite+aiosqlite:///:memory:` (requires `aiosqlite` dep).
        # Checking pyproject.toml -> `asyncpg` is used. `aiosqlite` not listed.
        # We can't use sqlite easily.
    
        # Fallback: Mock `session.execute` to return object with `.scalars().all()` returning empty list (no history).
        mock_result = MagicMock()
        mock_result.scalar_one_or_none.return_value = None # For session lookup -> triggers new session
        mock_result.scalars.return_value.all.return_value = [] # No history
    
        mock_session.execute.return_value = mock_result
    
        # Running the service
        response = await mock_agent_service.handle_request(request, session=mock_session)
    
        # Verification
>       assert "Hello World!" in response.response
E       AssertionError: assert 'Hello World!' in 'yes'
E        +  where 'yes' = AgentResponse(conversation_id='8bc24447-2631-44ea-85ff-cbfe5955d000', response='yes', created_at=datetime.datetime(2025, 12, 23, 15, 0, 2, 877140, tzinfo=datetime.timezone.utc), messages=[AgentMessage(role='user', content='Read /tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'), AgentMessage(role='assistant', content='yes')], steps=[{'type': 'plan', 'status': 'created', 'description': 'Read the file and answer.', 'plan': {'steps': [{'id': 'step-1', 'label': 'Read File', 'executor': 'agent', 'action': 'tool', 'tool': 'read_file', 'args': {'path': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'}, 'description': None, 'provider': None}, {'id': 'step-2', 'label': 'Answer', 'executor': 'litellm', 'action': 'completion', 'tool': None, 'args': {'model': 'mock-model'}, 'description': None, 'provider': None}], 'description': 'Read the file and answer.'}, 'trace': {'trace_id': None, 'span_id': None}}, {'type': 'plan_step', 'id': 'step-1', 'label': 'Read File', 'action': 'tool', 'executor': 'agent', 'tool': 'read_file', 'status': 'missing', 'trace': {'trace_id': None, 'span_id': None}, 'result': {'name': 'read_file', 'status': 'missing'}, 'decision': 'adjust'}, {'type': 'plan_step', 'id': 'step-2', 'label': 'Answer', 'action': 'completion', 'executor': 'litellm', 'tool': None, 'status': 'ok', 'trace': {'trace_id': None, 'span_id': None}, 'result': {'completion': 'yes', 'model': 'mock-model'}, 'decision': 'ok'}, {'type': 'completion', 'provider': 'litellm', 'model': 'mock-model', 'status': 'ok', 'plan_step_id': 'step-2', 'trace': {'trace_id': None, 'span_id': None}}], metadata={'cwd': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0', 'plan': {'steps': [{'id': 'step-1', 'label': 'Read File', 'executor': 'agent', 'action': 'tool', 'tool': 'read_file', 'args': {'path': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'}, 'description': None, 'provider': None}, {'id': 'step-2', 'label': 'Answer', 'executor': 'litellm', 'action': 'completion', 'tool': None, 'args': {'model': 'mock-model'}, 'description': None, 'provider': None}], 'description': 'Read the file and answer.'}, 'tool_results': [{'name': 'read_file', 'status': 'missing'}], 'trace': {'trace_id': None, 'span_id': None}}).response

src/core/tests/test_agent_scenarios.py:144: AssertionError
=============================== warnings summary ===============================
../../../../.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/litellm/types/llms/anthropic.py:531
  /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/litellm/types/llms/anthropic.py:531: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class AnthropicResponseContentBlockToolUse(BaseModel):

../../../../.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/litellm/types/rag.py:181
  /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/litellm/types/rag.py:181: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class RAGIngestRequest(BaseModel):

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:132: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(db_session)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:198: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/observability/tracing.py:214: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    span.set_attribute("span.end_time", datetime.utcnow().isoformat())

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:423: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/dev/ai-agent-platform/services/agent/src/core/core/service.py:461: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    session.add(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

src/core/tests/test_agent_scenarios.py::test_run_tool_flow
  /home/magnus/.cache/pypoetry/virtualenvs/ai-agent-platform-hIn861qG-py3.12/lib/python3.12/site-packages/pydantic/main.py:250: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED src/core/tests/test_agent_scenarios.py::test_run_tool_flow - AssertionError: assert 'Hello World!' in 'yes'
 +  where 'yes' = AgentResponse(conversation_id='8bc24447-2631-44ea-85ff-cbfe5955d000', response='yes', created_at=datetime.datetime(2025, 12, 23, 15, 0, 2, 877140, tzinfo=datetime.timezone.utc), messages=[AgentMessage(role='user', content='Read /tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'), AgentMessage(role='assistant', content='yes')], steps=[{'type': 'plan', 'status': 'created', 'description': 'Read the file and answer.', 'plan': {'steps': [{'id': 'step-1', 'label': 'Read File', 'executor': 'agent', 'action': 'tool', 'tool': 'read_file', 'args': {'path': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'}, 'description': None, 'provider': None}, {'id': 'step-2', 'label': 'Answer', 'executor': 'litellm', 'action': 'completion', 'tool': None, 'args': {'model': 'mock-model'}, 'description': None, 'provider': None}], 'description': 'Read the file and answer.'}, 'trace': {'trace_id': None, 'span_id': None}}, {'type': 'plan_step', 'id': 'step-1', 'label': 'Read File', 'action': 'tool', 'executor': 'agent', 'tool': 'read_file', 'status': 'missing', 'trace': {'trace_id': None, 'span_id': None}, 'result': {'name': 'read_file', 'status': 'missing'}, 'decision': 'adjust'}, {'type': 'plan_step', 'id': 'step-2', 'label': 'Answer', 'action': 'completion', 'executor': 'litellm', 'tool': None, 'status': 'ok', 'trace': {'trace_id': None, 'span_id': None}, 'result': {'completion': 'yes', 'model': 'mock-model'}, 'decision': 'ok'}, {'type': 'completion', 'provider': 'litellm', 'model': 'mock-model', 'status': 'ok', 'plan_step_id': 'step-2', 'trace': {'trace_id': None, 'span_id': None}}], metadata={'cwd': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0', 'plan': {'steps': [{'id': 'step-1', 'label': 'Read File', 'executor': 'agent', 'action': 'tool', 'tool': 'read_file', 'args': {'path': '/tmp/pytest-of-magnus/pytest-32/test_run_tool_flow0/hello.txt'}, 'description': None, 'provider': None}, {'id': 'step-2', 'label': 'Answer', 'executor': 'litellm', 'action': 'completion', 'tool': None, 'args': {'model': 'mock-model'}, 'description': None, 'provider': None}], 'description': 'Read the file and answer.'}, 'tool_results': [{'name': 'read_file', 'status': 'missing'}], 'trace': {'trace_id': None, 'span_id': None}}).response
======================== 1 failed, 16 warnings in 0.04s ========================
