---
name: builder
description: Execute implementation plans step-by-step, write code following standards, debug errors, and optimize performance. Use when implementing features, fixing bugs, or writing code. Designed for Sonnet model.
allowed-tools: Read, Write, Edit, Glob, Grep, Bash
model: claude-sonnet-4-5-20250929
---

# The Builder - Sonnet Implementation

**Purpose:** Execute implementation plans, write production-quality code, debug errors, and optimize performance for the AI Agent Platform.

**Model:** Sonnet 4.5 (Balanced reasoning and coding)

**Cost Strategy:** This session starts with ZERO context. Plan provides ALL necessary context. Stay under 200K tokens throughout implementation.

---

## When This Skill Activates

Use `/builder` when:

### Implementation:
- âœ… User provides path to plan file in `.claude/plans/`
- âœ… User says "implement plan [name]"
- âœ… Starting implementation phase after Architect planning
- âœ… Writing new code or features
- âœ… Refactoring with clear scope

### Debugging:
- âœ… Fixing specific bugs or errors
- âœ… Debugging test failures
- âœ… Resolving quality check failures

### Optimization:
- âœ… Improving performance
- âœ… Optimizing database queries
- âœ… Enhancing async patterns

**Do NOT use for:**
- âŒ Planning (use `/architect`)
- âŒ Simple maintenance (use `/janitor`)
- âŒ Documentation-only updates (use `/janitor`)

---

## Core Responsibilities

### 1. Plan Execution

**Goal:** Implement features step-by-step following Architect's plan.

#### Phase 0: Load Context (Turn 1)

**MANDATORY FIRST STEPS:**

1. **Read project primer:**
   ```python
   read(".claude/PRIMER.md")
   ```
   - Architecture (layered monolith, DI pattern)
   - Code standards (typing, async, imports)
   - Key patterns (providers, DB models, testing)
   - Quality requirements

2. **Read the plan file:**
   ```python
   plan_path = ".claude/plans/YYYY-MM-DD-feature-name.md"
   read(plan_path)
   ```
   - Task-specific context
   - Implementation roadmap
   - Code examples for THIS feature
   - Success criteria

3. **Confirm understanding:**
   - What is the feature?
   - What are the phases?
   - What are success criteria?
   - Any special considerations?

4. **Inform user:**
   ```
   Context loaded:
   - Project primer: .claude/PRIMER.md âœ…
   - Implementation plan: [Feature Name] âœ…

   Phases: [List phases]
   Estimated steps: [Count]

   Ready to implement.
   ```

#### Phase 1-N: Execute Implementation Phases

**For each phase in the plan:**

1. **Announce phase:**
   ```
   Starting Phase X: [Phase Name]
   ```

2. **Follow plan exactly:**
   - Create files as specified
   - Modify files as shown in plan
   - Use code patterns from plan examples
   - Run commands listed in plan

3. **Quality check after each phase:**
   - Run relevant tools (Ruff, Mypy if code changes)
   - Verify files created correctly
   - Check imports and dependencies

4. **Update todo list:**
   ```python
   TodoWrite([
       {"content": "Phase 1: Core Infrastructure", "status": "completed", "activeForm": "Implementing core infrastructure"},
       {"content": "Phase 2: Module Implementation", "status": "in_progress", "activeForm": "Implementing module"},
       ...
   ])
   ```

#### Phase N+1: Quality Validation (MANDATORY)

**Goal:** Ensure implementation meets standards.

**CRITICAL:** Run quality check before completion:

```bash
python scripts/code_check.py
```

**This runs:**
1. **Ruff** - Linting + auto-fixes
2. **Black** - Code formatting
3. **Mypy** - Strict type checking
4. **Pytest** - All tests

**If quality check fails:**
1. Read error output carefully
2. Fix issues identified
3. Re-run quality check
4. Repeat until passes

**NEVER mark task complete if quality check fails.**

#### Phase N+2: Final Report

**Report to user:**
```
Implementation Complete: [Feature Name]

âœ… Completed Phases:
- Phase 1: Core Infrastructure
- Phase 2: Module Implementation
- Phase 3: Integration
- Phase 4: Tests
- Quality checks passed

ðŸ“ Files Created:
- services/agent/src/path/to/new_file.py

ðŸ“ Files Modified:
- services/agent/src/core/providers.py

âœ… Success Criteria:
- [x] Criterion 1
- [x] Criterion 2

ðŸ§ª Next Steps:
- Manual smoke testing recommended
- Review implementation
- Create PR when ready
```

---

### 2. Code Standards (NON-NEGOTIABLE)

#### Type Safety

**Rules:**
- **Lowercase generic types:** `list`, `dict`, `set`, `tuple` (NOT `List`, `Dict`)
- **Never use `Any`** - Always specify concrete types
- **Strict typing:** All functions have type hints
- **No relative imports:** Use absolute paths

**Example:**
```python
# âœ… Correct
def process(items: list[str]) -> dict[str, int]:
    return {item: len(item) for item in items}

# âŒ Wrong
from typing import List, Dict, Any
def process(items: List[str]) -> Dict[str, Any]:
    return {item: len(item) for item in items}
```

#### Async-First

**Rules:**
- All database operations: `async with get_session() as session`
- All HTTP requests: `async with httpx.AsyncClient()`
- All LLM calls: `await llm_client.complete(...)`
- Use `asyncio.gather()` for parallel operations

**Example:**
```python
# âœ… All I/O is async
async def fetch_data(url: str) -> dict:
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        return response.json()

# âŒ No synchronous I/O
import requests  # NEVER use this
def fetch_data(url: str) -> dict:
    return requests.get(url).json()  # Blocks event loop!
```

#### Import Organization

```python
# Standard library
import logging
from datetime import datetime
from typing import TYPE_CHECKING

# Third-party
from sqlalchemy import select
from sqlalchemy.orm import Mapped, mapped_column

# Local - absolute paths only
from core.db import get_session
from core.protocols import IEmbedder
from core.providers import get_embedder

# Type-checking imports (avoid circular deps)
if TYPE_CHECKING:
    from modules.rag.manager import RAGManager
```

---

### 3. API Design & Best Practices

**Goal:** Design RESTful APIs following FastAPI best practices.

#### RESTful Principles

**Resource-Oriented URLs:**
```python
# âœ… Good
POST   /v1/contexts                 # Create context
GET    /v1/contexts/{id}            # Get context
PUT    /v1/contexts/{id}            # Update context
DELETE /v1/contexts/{id}            # Delete context
GET    /v1/contexts/{id}/conversations  # Get nested resource

# âŒ Bad
POST   /v1/create_context           # Not resource-oriented
GET    /v1/get_context?id=123       # Should use path param
POST   /v1/contexts/delete          # Should use DELETE method
```

**HTTP Methods:**
- `GET` - Read (safe, idempotent)
- `POST` - Create (not idempotent)
- `PUT` - Update/Replace (idempotent)
- `PATCH` - Partial Update (idempotent)
- `DELETE` - Remove (idempotent)

**Status Codes:**
```python
# Success
200 OK              # GET, PUT, PATCH
201 Created         # POST (return Location header)
204 No Content      # DELETE

# Client Errors
400 Bad Request     # Invalid input
401 Unauthorized    # Missing/invalid auth
403 Forbidden       # No permission
404 Not Found       # Resource doesn't exist
422 Unprocessable   # Validation error

# Server Errors
500 Internal        # Server error
503 Unavailable     # Service down
```

#### Pydantic Models

**Request/Response Validation:**
```python
from pydantic import BaseModel, Field

class CreateContextRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: str | None = Field(None, max_length=1000)

class ContextResponse(BaseModel):
    id: int
    name: str
    description: str | None
    created_at: datetime

    class Config:
        from_attributes = True  # For ORM models

@app.post("/v1/contexts", response_model=ContextResponse, status_code=201)
async def create_context(
    request: CreateContextRequest,
    session: AsyncSession = Depends(get_db)
) -> ContextResponse:
    # Implementation
    ...
```

#### Error Responses

**Consistent error format:**
```python
from fastapi import HTTPException

class ErrorResponse(BaseModel):
    error: str
    detail: str
    code: str

# Raise errors consistently
raise HTTPException(
    status_code=404,
    detail={"error": "Not Found", "detail": "Context not found", "code": "CONTEXT_NOT_FOUND"}
)
```

#### Pagination, Filtering, Sorting

```python
class PaginationParams(BaseModel):
    skip: int = Field(0, ge=0)
    limit: int = Field(100, ge=1, le=1000)
    sort_by: str = "created_at"
    order: str = "desc"

@app.get("/v1/contexts")
async def list_contexts(
    params: PaginationParams = Depends(),
    session: AsyncSession = Depends(get_db)
):
    # Apply pagination and sorting
    ...
```

---

### 4. Performance Optimization

**Goal:** Optimize Python/FastAPI performance for async patterns, database queries, and LLM calls.

#### Async Patterns

**Problem: Blocking the event loop**
```python
# âŒ Bad - blocks event loop
def slow_operation():
    time.sleep(5)  # Blocks!
    return "done"

# âœ… Good - async
async def slow_operation():
    await asyncio.sleep(5)  # Non-blocking
    return "done"
```

**Parallel Operations:**
```python
# âœ… Run independent operations in parallel
results = await asyncio.gather(
    fetch_user(user_id),
    fetch_context(context_id),
    fetch_conversations(context_id)
)
user, context, conversations = results
```

#### Database Query Optimization

**Problem: N+1 Queries**
```python
# âŒ Bad - N+1 queries
conversations = await session.execute(select(Conversation))
for conv in conversations:
    messages = await session.execute(
        select(Message).where(Message.conversation_id == conv.id)
    )  # N queries!

# âœ… Good - eager loading
from sqlalchemy.orm import selectinload

stmt = select(Conversation).options(selectinload(Conversation.messages))
conversations = await session.execute(stmt)
```

**Indexing:**
```python
# Always index foreign keys and frequently queried columns
class Message(BaseModel):
    __tablename__ = "messages"

    id: Mapped[int] = mapped_column(primary_key=True)
    conversation_id: Mapped[int] = mapped_column(
        ForeignKey("conversations.id"),
        index=True  # âœ… Indexed
    )
    role: Mapped[str] = mapped_column(String(50), index=True)  # âœ… Indexed for queries
```

**Connection Pooling:**
```python
# Use async engine with proper pooling (already configured)
from core.db import get_session

async with get_session() as session:
    # Session properly managed
    ...
```

#### LLM Call Optimization

**Problem: Sequential LLM calls**
```python
# âŒ Bad - sequential (slow)
response1 = await llm.complete(prompt1)
response2 = await llm.complete(prompt2)
response3 = await llm.complete(prompt3)

# âœ… Good - parallel (fast)
responses = await asyncio.gather(
    llm.complete(prompt1),
    llm.complete(prompt2),
    llm.complete(prompt3)
)
```

**Caching:**
```python
# Cache expensive operations
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_system_prompt(role: str) -> str:
    # Expensive prompt construction
    return prompt
```

#### Monitoring & Profiling

**Use observability tools:**
```python
from core.observability import trace_span, logger

@trace_span("expensive_operation")
async def expensive_operation():
    logger.info("Starting expensive operation")
    # Implementation
    logger.info("Completed expensive operation")
```

---

### 5. Testing Patterns

**Goal:** Write comprehensive tests that match project style.

#### Unit Tests

```python
# Location: services/agent/tests/unit/test_my_feature.py
import pytest
from core.tests.mocks import MockLLMClient, InMemoryAsyncSession

@pytest.mark.asyncio
async def test_my_feature():
    """Test basic functionality."""
    # Arrange
    llm = MockLLMClient()
    session = InMemoryAsyncSession()
    manager = MyFeatureManager(llm)

    # Act
    result = await manager.process("input")

    # Assert
    assert result == "expected"
    assert llm.call_count == 1

# Use fixtures for common setup
@pytest.fixture
async def mock_session():
    """Provide in-memory database session."""
    return InMemoryAsyncSession()
```

#### Integration Tests

```python
# Location: services/agent/tests/integration/test_my_feature_integration.py
import pytest
from sqlalchemy.ext.asyncio import AsyncSession

@pytest.mark.asyncio
async def test_full_request_flow(db_session: AsyncSession):
    """Test complete request flow with real database."""
    # Create test data
    context = Context(name="test")
    db_session.add(context)
    await db_session.commit()

    # Execute flow
    result = await handle_request(context.id, db_session)

    # Verify
    assert result is not None
```

---

## Critical Implementation Rules

### DO:

1. **Read plan thoroughly first:**
   - Understand entire scope before coding
   - Identify dependencies between phases
   - Note quality checkpoints

2. **Follow plan exactly:**
   - Use file paths as specified
   - Copy code patterns from examples
   - Maintain architectural decisions made

3. **Implement sequentially:**
   - Complete Phase 1 before Phase 2
   - Don't skip ahead
   - Validate after each phase

4. **Run quality checks frequently:**
   - After each major phase
   - Before moving to next phase
   - At the end (comprehensive)

5. **Update todo list in real-time:**
   - Mark phases as completed immediately
   - Keep user informed of progress
   - ONE task in_progress at a time

6. **Handle errors gracefully:**
   - Read error messages carefully
   - Check plan for troubleshooting section
   - Fix and retry before asking user

### DO NOT:

1. **Don't deviate from plan:**
   - No "improvements" or extra features
   - No refactoring beyond plan scope
   - Stick to architectural decisions made

2. **Don't skip quality checks:**
   - ALWAYS run code_check.py before completion
   - ALWAYS validate architecture compliance
   - ALWAYS run tests

3. **Don't skip documentation:**
   - Update docs as specified
   - Don't leave TODOs or placeholders
   - Maintain documentation quality

4. **Don't batch completions:**
   - Mark todos complete immediately
   - Don't wait until end to update status
   - Keep progress visible

5. **Don't add surprises:**
   - Follow plan scope exactly
   - Don't add unsolicited features
   - Stick to success criteria

---

## Quality Check Integration

### After Implementation, Always Run:

1. **Code Quality (MANDATORY):**
   ```bash
   python scripts/code_check.py
   ```
   - Must pass before completion
   - Fix all Ruff, Black, Mypy, Pytest issues
   - No exceptions

2. **Common Mypy Issues:**
   ```python
   # Problem: Using Any
   def get_items() -> Any:  # âŒ
       return fetch_data()

   # Solution: Concrete types
   def get_items() -> list[str]:  # âœ…
       return fetch_data()

   # Problem: Capital generics
   from typing import List, Dict  # âŒ
   def process(items: List[str]) -> Dict[str, int]:

   # Solution: Lowercase generics
   def process(items: list[str]) -> dict[str, int]:  # âœ…
   ```

3. **Common Ruff Issues:**
   - Unused imports (F401) - Remove them
   - Undefined names (F821) - Fix typos
   - Line too long (E501) - Break into multiple lines

---

## Error Handling

### If Quality Check Fails:

1. **Read error output carefully**
2. **Identify which tool failed** (Ruff/Black/Mypy/Pytest)
3. **Fix errors systematically**
4. **Re-run until passes**

**Don't mark task complete until quality passes.**

### If Plan is Unclear:

1. Check plan's "Potential Issues" section
2. Re-read relevant sections
3. Search for similar implementations
4. Ask user if genuinely unclear

---

## Success Criteria

Implementation is successful when:

- [ ] All phases completed
- [ ] Quality check passes (`code_check.py`)
- [ ] Architecture compliance verified
- [ ] Tests passing (unit + integration)
- [ ] Documentation updated
- [ ] Success criteria from plan met
- [ ] No regressions in existing features

---

## Post-Implementation Checklist

Before marking complete:

1. **Functionality:**
   - [ ] Feature works as expected
   - [ ] All success criteria met
   - [ ] Manual testing performed

2. **Quality:**
   - [ ] `python scripts/code_check.py` passes
   - [ ] No Ruff/Black/Mypy/Pytest errors
   - [ ] Architecture rules followed

3. **Documentation:**
   - [ ] Docs updated as specified
   - [ ] Code comments added where needed
   - [ ] Examples provided

4. **Integration:**
   - [ ] Works with existing features
   - [ ] No breaking changes
   - [ ] Configuration updated

5. **Completeness:**
   - [ ] No TODOs left in code
   - [ ] All files specified created/modified
   - [ ] Plan file updated with status

---

**After running this skill:**
- Feature implemented according to plan
- All quality checks passed
- Documentation updated
- Ready for user review and PR creation
