name: ai-agent-stack

services:
  agent:
    build:
      context: .
      dockerfile: Dockerfile.agent
    env_file:
      - .env
    environment:
      AGENT_ENVIRONMENT: ${AGENT_ENVIRONMENT:-production}
    ports:
      - "8000:8000"
    depends_on:
      litellm:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 5s
      retries: 5

  litellm:
    image: ghcr.io/berriai/litellm:latest
    environment:
      LITELLM_LOGGING: warn
      LITELLM_PORT: 4000
      LITELLM_PROXY_MODEL: ollama/llama3
      OLLAMA_API_BASE: http://ollama:11434
    ports:
      - "4000:4000"
    depends_on:
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 5s
      retries: 5

  ollama:
    image: ollama/ollama:latest
    environment:
      OLLAMA_HOST: 0.0.0.0
    # GPU runtimes are provided via docker-compose.gpu.yml to keep the base stack
    # compatible with docker compose config validation in CI.
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "--version"]
      interval: 1m
      timeout: 10s
      retries: 3

  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - qdrant-data:/qdrant/storage
    ports:
      - "6333:6333"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 5s
      retries: 5

  webui:
    image: ghcr.io/open-webui/open-webui:latest
    environment:
      WEBUI_NAME: "AI Agent Platform"
      LITELLM_URL: http://agent:8000
      OPENAI_API_BASE_URL: http://agent:8000
      OPENAI_API_KEY: ${OPENWEBUI_OPENAI_API_KEY:-agent-placeholder}
    ports:
      - "3000:8080"
    depends_on:
      agent:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 5s
      retries: 5

  webfetch:
    build:
      context: fetcher
    environment:
      FETCHER_PORT: 8081
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 5s
      retries: 5

volumes:
  ollama-models:
  qdrant-data:
