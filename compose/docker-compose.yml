services:
  searxng:
    container_name: searxng
    image: searxng/searxng:latest
    restart: unless-stopped
    ports:
      - "${SEARXNG_PORT:-8080}:8080"
    environment:
      - BASE_URL=http://searxng:${SEARXNG_PORT:-8080}/
      - SEARXNG_SECRET=change_me
      - AUTOCOMPLETE=duckduckgo
      - RESULT_PROXY=false
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://searxng:${SEARXNG_PORT:-8080}/"]
      interval: 10s
      timeout: 3s
      retries: 10

  webfetch:
    container_name: webfetch
    build:
      context: ../fetcher
      dockerfile: Dockerfile
    restart: unless-stopped
    environment:
      - SEARXNG_URL=http://searxng:8080
      - LITELLM_BASE=http://litellm:4000
      - DEFAULT_MODEL=local/llama3-8b
      - REQUEST_TIMEOUT=15
      - MAX_WORKERS=4
    ports:
      - "${FETCHER_PORT:-8081}:8081"
    depends_on:
      - searxng
      - litellm
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8081/health"]
      interval: 10s
      timeout: 3s
      retries: 10

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # 'deploy' ignoreras av vanlig Compose, kvar för Swarm/komp.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  qdrant:
    container_name: qdrant
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "${QDRANT_PORT:-6333}:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-stable
    restart: unless-stopped
    command: ["--config","/app/config.yaml","--port","4000"] 
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    volumes:
      - ../litellm/config.yaml:/app/config.yaml:ro
    ports:
      - "${LITELLM_PORT:-4000}:4000"
    depends_on: [ollama]



  openwebui:
    container_name: openwebui
    image: ghcr.io/open-webui/open-webui:latest
    restart: unless-stopped
    environment:
      - OPENAI_API_BASE=http://litellm:4000
      - OPENAI_API_KEY=dummy
      - WEBUI_AUTH=True
      - ENABLE_SIGNUP=true            # sätt False när första kontot är skapat
    volumes:
      - ../openwebui/data:/app/backend/data
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"
    depends_on: [litellm]

  n8n:
    container_name: n8n
    image: n8nio/n8n:latest
    restart: unless-stopped
    environment:
      - TZ=${TIMEZONE:-Europe/Stockholm}
      - GENERIC_TIMEZONE=${TIMEZONE:-Europe/Stockholm}
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_HOST=${N8N_HOST:-localhost}
    ports:
      - "${N8N_PORT:-5678}:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 3s
      retries: 10

volumes:
  ollama_data:
  qdrant_data:
  n8n_data:
